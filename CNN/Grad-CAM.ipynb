{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38497f0-1062-49eb-8b40-97223a7d3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = []\n",
    "        self.gradients = []\n",
    "\n",
    "    def _save_gradient(self, grad):\n",
    "        self.gradients.append(grad)\n",
    "\n",
    "    def _get_activations_gradient(self):\n",
    "        grads = self.gradients[-1]\n",
    "        activations = self.activations[-1]\n",
    "        return grads, activations\n",
    "\n",
    "    def _get_linear_weights(self, grads):\n",
    "        return torch.mean(grads, axis=(2, 3))\n",
    "\n",
    "    def _get_conv_weights(self, grads):\n",
    "        return torch.mean(grads, axis=(2, 3))\n",
    "\n",
    "    def _get_gradcam_image(self, conv_output, grads):\n",
    "        weights = self._get_conv_weights(grads)[0]\n",
    "        cam = torch.zeros(conv_output.shape[1:], dtype=torch.float32, device=conv_output.device)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * conv_output[i, :, :]\n",
    "        cam = F.relu(cam)\n",
    "        cam = F.interpolate(cam.unsqueeze(0).unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        return cam.squeeze().cpu().numpy()\n",
    "\n",
    "    def __call__(self, x, class_idx=None, device='cpu', height=None, width=None):\n",
    "        x = x.to(device)\n",
    "        self.activations = []\n",
    "        self.gradients = []\n",
    "        output = x\n",
    "        for name, module in self.model.named_children():\n",
    "            if name == self.target_layer:\n",
    "                output.register_hook(self._save_gradient)\n",
    "                self.activations.append(output)\n",
    "                output = output.detach()\n",
    "            elif isinstance(module, torch.nn.Sequential) or isinstance(module, torch.nn.ModuleList):\n",
    "                for _, module in module.named_children():\n",
    "                    output = module(output)\n",
    "                    if isinstance(module, torch.nn.ReLU):\n",
    "                        output.register_hook(self._save_gradient)\n",
    "                        self.activations.append(output)\n",
    "                        output = output.detach()\n",
    "            else:\n",
    "                output = module(output)\n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(output)\n",
    "        self.logits = output[:, class_idx]\n",
    "        self.logits.backward(retain_graph=True)\n",
    "        grads, activations = self._get_activations_gradient()\n",
    "        conv_output = activations.detach()\n",
    "        cam = self._get_gradcam_image(conv_output, grads)\n",
    "        upsampled_cam = cv2.resize(cam, (width, height))\n",
    "        return upsampled_cam\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1542aea-daac-472b-8179-384e7276d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Cargar los pesos de EfficientNetB2\n",
    "model_weights = torch.load(\"/Users/guill/Escritorio/CNN/best_model_EfficientNetB2_FrozenLast8.pth\")\n",
    "\n",
    "# Construir el modelo EfficientNetB2 con los pesos cargados\n",
    "model = EfficientNet.from_pretrained('efficientnet-b2', weights_path=None)\n",
    "model.load_state_dict(model_weights)\n",
    "\n",
    "# Establecer el modelo en modo de evaluación\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1514f-5c3a-4d4e-ba7b-8c031c1cc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "gradcam = GradCAM(model, target_layer='layer4')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "img = cv2.imread(\"/Users/guill/Escritorio/TesisLisboa/archive/DFU/TestSet/test/Abnormal (63).jpg\")\n",
    "\n",
    "resized_img = cv2.resize(img, (224, 224))\n",
    "resized_part = torch.from_numpy(resized_img).permute(2, 0, 1).unsqueeze(0).float().div(255)\n",
    "cam = gradcam(resized_part.to(device), class_idx=0, device=device, height=img.shape[0], width=img.shape[1])\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + image.squeeze().permute(1, 2, 0).cpu().numpy() * 0.5\n",
    "result = result / np.max(result)\n",
    "cv2.imshow('Result', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96682658-b350-45d6-91c0-7fafe7dca49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "# Carga la imagen\n",
    "image = cv2.imread(\"/Users/guill/Escritorio/TesisLisboa/archive/DFU/TestSet/test/Abnormal (63).jpg\")\n",
    "\n",
    "# Divide la imagen en partes\n",
    "num_rows = 3\n",
    "num_cols = 3\n",
    "height, width = image.shape[:2]\n",
    "part_height = height // num_rows\n",
    "part_width = width // num_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0353e3-2859-4a58-b1b2-4471b1151a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \n",
    "    #otros modelos:\"resnet18\"\"alexnet\"\"vgg16\"\"densenet121\" ResNet18, ResNet34, ResNet50, ResNet101, ResNet152\n",
    "#VGG11, VGG13, VGG16, VGG19\n",
    "#DenseNet121, DenseNet161, DenseNet169, DenseNet201\n",
    "#InceptionV3, InceptionResNetV2\n",
    "#MobileNetV2, MobileNetV3\n",
    "#EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7\n",
    "    \"model\": \"densenet121\",\n",
    "    \"device\": \"cpu\",  # usar 'cuda' para usar la GPU\n",
    "    \"lr\": 0.001, #cambiar a 0.01\n",
    "    \"batch_size\": 64,#cambiar a 32\n",
    "    \"num_workers\": 0,\n",
    "    \"epochs\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b70b1-e958-4d19-9913-30777ae07fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientNetB2\n",
    "#congelo las ultimas 8 capa\n",
    "\n",
    "import torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Cargar la red EfficientNet pre-entrenada\n",
    "model = EfficientNet.from_pretrained('efficientnet-b2')\n",
    "\n",
    "\n",
    "\n",
    "# Congelar las primeras capas\n",
    "for name, param in model.named_parameters():\n",
    "    if not name.startswith('_conv_head.') and not name.startswith('_fc.') and not name.startswith('_bn1.') and not name.startswith('_blocks.22') and not name.startswith('_blocks.21') and not name.startswith('_blocks.20') and not name.startswith('_blocks.19') and not name.startswith('_blocks.18') and not name.startswith('_blocks.17') and not name.startswith('_blocks.16') and not name.startswith('_blocks.15'): \n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "# Reemplazar el clasificador lineal con uno personalizado\n",
    "num_ftrs = model._fc.in_features\n",
    "model._fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "# Mover el modelo y el criterio al dispositivo especificado\n",
    "model = model.to(params[\"device\"])\n",
    "criterion = nn.BCEWithLogitsLoss().to(params[\"device\"])\n",
    "\n",
    "# Definir el optimizador y ajustar solo los parámetros que no están congelados\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=params[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c04ed-2d2e-4e57-affe-0539443e38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el modelo\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model_EfficientNetB2_FrozenLast8.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9e641-66c7-466e-a990-ec7b4a6fda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una matriz para almacenar los mapas de activación GradCAM de cada parte\n",
    "gradcam_matrix = np.zeros((num_rows, num_cols, part_height, part_width))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f8a1e-d367-4da9-82ca-3a68d7581b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(max_size=160),\n",
    "        A.CenterCrop(height=128, width=128),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(), #solo lo quito si en la funcion de arriba estoy ya creando el tensor\n",
    "        \n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682908a-63f6-4ba5-a1b6-f10f0fd580c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliza GradCAM para obtener los mapas de activación de cada parte de la imagen\n",
    "gradcam = GradCAM(model=model, target_layer=getattr(model._blocks[22], '_expand_conv'))\n",
    "transform = test_transform\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    " \n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        part = image[i*part_height:(i+1)*part_height, j*part_width:(j+1)*part_width]\n",
    "        resized_part = cv2.resize(part, (224, 224))\n",
    "        \n",
    "        resized_part = test_transform(image=resized_part)['image']\n",
    "        \n",
    "        resized_part = resized_part.unsqueeze(0)\n",
    "        resized_part.requires_grad_()\n",
    "        with torch.no_grad():\n",
    "            logits = model(resized_part)\n",
    "        cam = gradcam(resized_part, device=device, height=part_height, width=part_width)\n",
    "        cam.backward(retain_graph=True)\n",
    "\n",
    "        upsampled_cam = cv2.resize(cam, (part_width, part_height))\n",
    "        gradcam_matrix[i,j] = upsampled_cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dfa8ff-fab4-4b44-b845-1eb18e4e7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une los mapas de activación GradCAM para crear una imagen completa de GradCAM\n",
    "gradcam_image = np.zeros_like(image)\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        part = gradcam_matrix[i,j]\n",
    "        x = j * part_width\n",
    "        y = i * part_height\n",
    "        gradcam_image[y:y+part_height, x:x+part_width] = part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f8415-adfc-40c8-869b-2e97a148250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica la función de activación ReLU para obtener únicamente las partes activadas\n",
    "heatmap = np.maximum(gradcam_image, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2959d-302b-474a-97c2-4983917ab1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza el mapa de calor entre 0 y 1\n",
    "heatmap /= heatmap.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb2ecf-cce3-462b-bc53-08ddf04dd5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una imagen en color a partir del mapa de calor y la imagen original\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "output_image = heatmap * 0.5 + image * 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dabd8f0-be73-4d0b-892b-3c5351cf3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra la imagen resultante\n",
    "cv2.imshow(\"GradCAM\", output_image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f6034-0855-421e-a841-18fe77427e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
